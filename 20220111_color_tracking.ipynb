{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20220111_color_tracking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh7STJazxWoG+Bdoi9WfqY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwhc441/mask_police/blob/main/20220111_color_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gmQXBE4-Q7JP"
      },
      "outputs": [],
      "source": [
        "#物体検出（輪郭）\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        " \n",
        "# 画像から輪郭を検出する関数\n",
        "def contours(img):\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                  # グレースケール化\n",
        "    ret, img_binary = cv2.threshold(img_gray,                         # 二値化\n",
        "                                    60, 255,                          # 二値化のための閾値60(調整要)\n",
        "                                    cv2.THRESH_BINARY)\n",
        "    contours, hierarchy = cv2.findContours(img_binary,                # 輪郭検出\n",
        "                                           cv2.RETR_EXTERNAL,         # 外側の輪郭のみ抽出\n",
        "                                           cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = np.array(contours)                                     # 輪郭情報をndarrayに変換\n",
        "    x = np.mean(contours[0].T[0, 0])                                  # 輪郭のx方向平均値を算出\n",
        "    y = np.mean(contours[0].T[1, 0])                                  # 輪郭のy方向平均値を算出\n",
        "    return x, y\n",
        " \n",
        "movie = cv2.VideoCapture('video.mp4')                                 # 動画ファイルの読み込み\n",
        " \n",
        "# 動画ファイル保存用の設定\n",
        "fps = int(movie.get(cv2.CAP_PROP_FPS))                                # 動画のFPSを取得\n",
        "w = int(movie.get(cv2.CAP_PROP_FRAME_WIDTH))                          # 動画の横幅を取得\n",
        "h = int(movie.get(cv2.CAP_PROP_FRAME_HEIGHT))                         # 動画の縦幅を取得\n",
        "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')                   # 動画保存時のfourcc設定（mp4用）\n",
        "video = cv2.VideoWriter('video_out.mp4', fourcc, fps, (w, h), True)   # 動画の仕様（ファイル名、fourcc, FPS, サイズ, カラー）\n",
        " \n",
        "# ファイルからフレームを1枚ずつ取得して動画処理後に保存する\n",
        "x_list = []\n",
        "y_list = []\n",
        "while True:\n",
        "    ret, frame = movie.read()                                         # フレームを取得\n",
        " \n",
        "    # フレームが取得できない場合はループを抜ける\n",
        "    if not ret:\n",
        "        break\n",
        " \n",
        "    x, y = contours(frame)                                            # 輪郭検出から物体中心を算出\n",
        " \n",
        "    frame = cv2.circle(frame, (int(x), int(y)), 30, (0, 255, 0), 3)   # 検出した位置にサークル描画\n",
        " \n",
        "    video.write(frame)  # 動画を保存する\n",
        "    x_list.append(x)\n",
        "    y_list.append(y)\n",
        " \n",
        "# 動画オブジェクト解放\n",
        "movie.release()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!pip install bottle\n",
        "!pip install bottle_websocket\n",
        "!pip install gevent\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "def use_cam(url, quality=0.8):\n",
        "  print(\"start camera\")\n",
        "  js = Javascript('''\n",
        "    async function useCam(url, quality) {\n",
        "      const div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "      //video element\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'None';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      //canvas for display. frame rate is depending on display size and jpeg quality.\n",
        "      display_size = 500 \n",
        "      const src_canvas = document.createElement('canvas');\n",
        "      src_canvas.width  = display_size;\n",
        "      src_canvas.height = display_size * video.videoHeight / video.videoWidth;\n",
        "      const src_canvasCtx = src_canvas.getContext('2d');\n",
        "      src_canvasCtx.translate(src_canvas.width, 0);\n",
        "      src_canvasCtx.scale(-1, 1);\n",
        "      div.appendChild(src_canvas);\n",
        "\n",
        "      const dst_canvas = document.createElement('canvas');\n",
        "      dst_canvas.width  = src_canvas.width;\n",
        "      dst_canvas.height = src_canvas.height;\n",
        "      const dst_canvasCtx = dst_canvas.getContext('2d');\n",
        "      div.appendChild(dst_canvas);\n",
        "\n",
        "      //exit button\n",
        "      const btn_div = document.createElement('div');\n",
        "      document.body.appendChild(btn_div);\n",
        "      const exit_btn = document.createElement('button');\n",
        "      exit_btn.textContent = 'Exit';\n",
        "      var exit_flg = true\n",
        "      exit_btn.onclick = function() {exit_flg = false};\n",
        "      btn_div.appendChild(exit_btn);\n",
        "\n",
        "      //log\n",
        "      let jsLog = function(abc) {\n",
        "        document.querySelector(\"#output-area\").appendChild(document.createTextNode(`${abc} `));\n",
        "      }\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      //for websocket connection.\n",
        "      var connection = 0\n",
        "      var flag_count = 0\n",
        "      var send_flg = false\n",
        "\n",
        "      // loop\n",
        "      _canvasUpdate();\n",
        "      async function _canvasUpdate() {\n",
        "            flag_count += 1\n",
        "\n",
        "            //wait until websocket launch\n",
        "            if (flag_count == 200){\n",
        "                connection = new WebSocket(url); \n",
        "                jsLog(\"Connect_start\")\n",
        "            }\n",
        "            if (flag_count == 300){\n",
        "                connection.onmessage = function(e) {\n",
        "                    var image = new Image()\n",
        "                    image.src = e.data;\n",
        "                    image.onload = function(){dst_canvasCtx.drawImage(image, 0, 0)}\n",
        "                    send_flg=false\n",
        "                };\n",
        "                jsLog(\"Set_recieve\")\n",
        "            }\n",
        "            if(flag_count > 400){\n",
        "                //resize to reduce file size\n",
        "                src_canvasCtx.drawImage(video, 0, 0, video.videoWidth, video.videoHeight, 0, 0, src_canvas.width, src_canvas.height);\n",
        "                const img = src_canvas.toDataURL('image/jpeg', quality);\n",
        "                if (send_flg==false){\n",
        "                    connection.send(img);\n",
        "                    send_flg = true\n",
        "                }\n",
        "            }\n",
        "          if (exit_flg){\n",
        "              requestAnimationFrame(_canvasUpdate);   \n",
        "          }else{\n",
        "              stream.getVideoTracks()[0].stop();\n",
        "              connection.close();\n",
        "          }\n",
        "      };\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('useCam(\"{}\", {})'.format(url, quality))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json;url = json.load(sys.stdin)['tunnels'][0]['public_url'];print(url);f = open('url.txt', 'w');f.write(url);f.close()\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTS20Wl8RA_q",
        "outputId": "0e39cf20-26bd-430d-e7fb-0ed0076c53be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-11 08:56:20--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  7.95MB/s    in 1.7s    \n",
            "\n",
            "2022-01-11 08:56:22 (7.95 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Collecting bottle\n",
            "  Downloading bottle-0.12.19-py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 3.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: bottle\n",
            "Successfully installed bottle-0.12.19\n",
            "Collecting bottle_websocket\n",
            "  Downloading bottle-websocket-0.2.9.tar.gz (2.0 kB)\n",
            "Requirement already satisfied: bottle in /usr/local/lib/python3.7/dist-packages (from bottle_websocket) (0.12.19)\n",
            "Collecting gevent-websocket\n",
            "  Downloading gevent_websocket-0.10.1-py3-none-any.whl (22 kB)\n",
            "Collecting gevent\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 7.2 MB/s \n",
            "\u001b[?25hCollecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent->gevent-websocket->bottle_websocket) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent->gevent-websocket->bottle_websocket) (57.4.0)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Building wheels for collected packages: bottle-websocket\n",
            "  Building wheel for bottle-websocket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bottle-websocket: filename=bottle_websocket-0.2.9-py3-none-any.whl size=2349 sha256=1f3de5ec6b7eb930469580a65f33cf62089f7cc9466d300b85d9e1e50881caa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/b2/4e/64bf3902ad2756ffb308af8b68e859e3c57b860525021ca0fd\n",
            "Successfully built bottle-websocket\n",
            "Installing collected packages: zope.interface, zope.event, gevent, gevent-websocket, bottle-websocket\n",
            "Successfully installed bottle-websocket-0.2.9 gevent-21.12.0 gevent-websocket-0.10.1 zope.event-4.5.0 zope.interface-5.4.0\n",
            "Requirement already satisfied: gevent in /usr/local/lib/python3.7/dist-packages (21.12.0)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent) (4.5.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent) (57.4.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent) (1.1.2)\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Need adjustment\n",
        "lower_light_pink = np.array([168, 50, 50])\n",
        "upper_light_pink = np.array([188, 255, 255])\n",
        "\n",
        "_LOWER_COLOR = lower_light_pink\n",
        "_UPPER_COLOR = upper_light_pink\n",
        "\n",
        "def tracking():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    filter = ParticleFilter()\n",
        "    filter.initialize()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # Threshold the HSV image to get only a color\n",
        "        mask = cv2.inRange(hsv, _LOWER_COLOR, _UPPER_COLOR)\n",
        "\n",
        "        # Start Tracking\n",
        "        y, x = filter.filtering(mask)\n",
        "        frame = cv2.circle(frame, (int(x), int(y)), 10, (255, 0, 0), -1)\n",
        "\n",
        "        # origin is upper left\n",
        "        frame_size = frame.shape\n",
        "        print (\"position_x_rate\")\n",
        "        print (x/frame_size[1])\n",
        "        print (\"position_y_rate\")\n",
        "        print (y/frame_size[0])\n",
        "\n",
        "        for i in range(filter.SAMPLEMAX):\n",
        "            frame = cv2.circle(frame, (int(filter.X[i]), int(filter.Y[i])), 2, (0, 0, 255), -1)\n",
        "        cv2.imshow(\"frame\", frame)\n",
        "\n",
        "        if cv2.waitKey(20) & 0xFF == 27:\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "class ParticleFilter:\n",
        "    def __init__(self):\n",
        "        self.SAMPLEMAX = 1000\n",
        "        # frame.shape\n",
        "        self.height, self.width = 480, 640\n",
        "\n",
        "    def initialize(self):\n",
        "        self.Y = np.random.random(self.SAMPLEMAX) * self.height\n",
        "        self.X = np.random.random(self.SAMPLEMAX) * self.width\n",
        "\n",
        "    # Need adjustment for tracking object velocity\n",
        "    def modeling(self):\n",
        "        self.Y += np.random.random(self.SAMPLEMAX) * 200 - 100 # 2:1\n",
        "        self.X += np.random.random(self.SAMPLEMAX) * 200 - 100\n",
        "\n",
        "    def normalize(self, weight):\n",
        "        return weight / np.sum(weight)\n",
        "\n",
        "    def resampling(self, weight):\n",
        "        index = np.arange(self.SAMPLEMAX)\n",
        "        sample = []\n",
        "\n",
        "        # choice by weight \n",
        "        for i in range(self.SAMPLEMAX):\n",
        "            idx = np.random.choice(index, p=weight)\n",
        "            sample.append(idx)\n",
        "        return sample\n",
        "\n",
        "    def calcLikelihood(self, image):\n",
        "        # white space tracking \n",
        "        mean, std = 250.0, 10.0\n",
        "        intensity = []\n",
        "\n",
        "        for i in range(self.SAMPLEMAX):\n",
        "            y, x = self.Y[i], self.X[i]\n",
        "            if y >= 0 and y < self.height and x >= 0 and x < self.width:\n",
        "                intensity.append(image[int(y),int(x)])\n",
        "            else:\n",
        "                intensity.append(-1)\n",
        "\n",
        "        # normal distribution\n",
        "        weights = 1.0 / np.sqrt(2 * np.pi * std) * np.exp(-(np.array(intensity) - mean)**2 /(2 * std**2))\n",
        "        weights[intensity == -1] = 0\n",
        "        weights = self.normalize(weights)\n",
        "        return weights\n",
        "\n",
        "    def filtering(self, image):\n",
        "        self.modeling()\n",
        "        weights = self.calcLikelihood(image)\n",
        "        index = self.resampling(weights)\n",
        "        self.Y = self.Y[index]\n",
        "        self.X = self.X[index]\n",
        "        \n",
        "        # return COG\n",
        "        return np.sum(self.Y) / float(len(self.Y)), np.sum(self.X) / float(len(self.X))\n",
        "tracking()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "7adrW7tgTlCQ",
        "outputId": "eb87c21a-84fe-475f-bf1d-f37c742e053f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-50241b6c0f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# return COG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mtracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-50241b6c0f33>\u001b[0m in \u001b[0;36mtracking\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mhsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2HSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Threshold the HSV image to get only a color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OwtLDczhUXHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}