{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20220111_color_tracking_by_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7t1/uk3VvQrLtr5F87o2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwhc441/mask_police/blob/main/20220111_color_tracking_by_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmQXBE4-Q7JP"
      },
      "outputs": [],
      "source": [
        "#物体検出（輪郭）\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        " \n",
        "# 画像から輪郭を検出する関数\n",
        "def contours(img):\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                  # グレースケール化\n",
        "    ret, img_binary = cv2.threshold(img_gray,                         # 二値化\n",
        "                                    60, 255,                          # 二値化のための閾値60(調整要)\n",
        "                                    cv2.THRESH_BINARY)\n",
        "    contours, hierarchy = cv2.findContours(img_binary,                # 輪郭検出\n",
        "                                           cv2.RETR_EXTERNAL,         # 外側の輪郭のみ抽出\n",
        "                                           cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = np.array(contours)                                     # 輪郭情報をndarrayに変換\n",
        "    x = np.mean(contours[0].T[0, 0])                                  # 輪郭のx方向平均値を算出\n",
        "    y = np.mean(contours[0].T[1, 0])                                  # 輪郭のy方向平均値を算出\n",
        "    return x, y\n",
        " \n",
        "movie = cv2.VideoCapture('video.mp4')                                 # 動画ファイルの読み込み\n",
        " \n",
        "# 動画ファイル保存用の設定\n",
        "fps = int(movie.get(cv2.CAP_PROP_FPS))                                # 動画のFPSを取得\n",
        "w = int(movie.get(cv2.CAP_PROP_FRAME_WIDTH))                          # 動画の横幅を取得\n",
        "h = int(movie.get(cv2.CAP_PROP_FRAME_HEIGHT))                         # 動画の縦幅を取得\n",
        "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')                   # 動画保存時のfourcc設定（mp4用）\n",
        "video = cv2.VideoWriter('video_out.mp4', fourcc, fps, (w, h), True)   # 動画の仕様（ファイル名、fourcc, FPS, サイズ, カラー）\n",
        " \n",
        "# ファイルからフレームを1枚ずつ取得して動画処理後に保存する\n",
        "x_list = []\n",
        "y_list = []\n",
        "while True:\n",
        "    ret, frame = movie.read()                                         # フレームを取得\n",
        " \n",
        "    # フレームが取得できない場合はループを抜ける\n",
        "    if not ret:\n",
        "        break\n",
        " \n",
        "    x, y = contours(frame)                                            # 輪郭検出から物体中心を算出\n",
        " \n",
        "    frame = cv2.circle(frame, (int(x), int(y)), 30, (0, 255, 0), 3)   # 検出した位置にサークル描画\n",
        " \n",
        "    video.write(frame)  # 動画を保存する\n",
        "    x_list.append(x)\n",
        "    y_list.append(y)\n",
        " \n",
        "# 動画オブジェクト解放\n",
        "movie.release()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!pip install bottle\n",
        "!pip install bottle_websocket\n",
        "!pip install gevent\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "def use_cam(url, quality=0.8):\n",
        "  print(\"start camera\")\n",
        "  js = Javascript('''\n",
        "    async function useCam(url, quality) {\n",
        "      const div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "      //video element\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'None';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      //canvas for display. frame rate is depending on display size and jpeg quality.\n",
        "      display_size = 500 \n",
        "      const src_canvas = document.createElement('canvas');\n",
        "      src_canvas.width  = display_size;\n",
        "      src_canvas.height = display_size * video.videoHeight / video.videoWidth;\n",
        "      const src_canvasCtx = src_canvas.getContext('2d');\n",
        "      src_canvasCtx.translate(src_canvas.width, 0);\n",
        "      src_canvasCtx.scale(-1, 1);\n",
        "      div.appendChild(src_canvas);\n",
        "\n",
        "      const dst_canvas = document.createElement('canvas');\n",
        "      dst_canvas.width  = src_canvas.width;\n",
        "      dst_canvas.height = src_canvas.height;\n",
        "      const dst_canvasCtx = dst_canvas.getContext('2d');\n",
        "      div.appendChild(dst_canvas);\n",
        "\n",
        "      //exit button\n",
        "      const btn_div = document.createElement('div');\n",
        "      document.body.appendChild(btn_div);\n",
        "      const exit_btn = document.createElement('button');\n",
        "      exit_btn.textContent = 'Exit';\n",
        "      var exit_flg = true\n",
        "      exit_btn.onclick = function() {exit_flg = false};\n",
        "      btn_div.appendChild(exit_btn);\n",
        "\n",
        "      //log\n",
        "      let jsLog = function(abc) {\n",
        "        document.querySelector(\"#output-area\").appendChild(document.createTextNode(`${abc} `));\n",
        "      }\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      //for websocket connection.\n",
        "      var connection = 0\n",
        "      var flag_count = 0\n",
        "      var send_flg = false\n",
        "\n",
        "      // loop\n",
        "      _canvasUpdate();\n",
        "      async function _canvasUpdate() {\n",
        "            flag_count += 1\n",
        "\n",
        "            //wait until websocket launch\n",
        "            if (flag_count == 200){\n",
        "                connection = new WebSocket(url); \n",
        "                jsLog(\"Connect_start\")\n",
        "            }\n",
        "            if (flag_count == 300){\n",
        "                connection.onmessage = function(e) {\n",
        "                    var image = new Image()\n",
        "                    image.src = e.data;\n",
        "                    image.onload = function(){dst_canvasCtx.drawImage(image, 0, 0)}\n",
        "                    send_flg=false\n",
        "                };\n",
        "                jsLog(\"Set_recieve\")\n",
        "            }\n",
        "            if(flag_count > 400){\n",
        "                //resize to reduce file size\n",
        "                src_canvasCtx.drawImage(video, 0, 0, video.videoWidth, video.videoHeight, 0, 0, src_canvas.width, src_canvas.height);\n",
        "                const img = src_canvas.toDataURL('image/jpeg', quality);\n",
        "                if (send_flg==false){\n",
        "                    connection.send(img);\n",
        "                    send_flg = true\n",
        "                }\n",
        "            }\n",
        "          if (exit_flg){\n",
        "              requestAnimationFrame(_canvasUpdate);   \n",
        "          }else{\n",
        "              stream.getVideoTracks()[0].stop();\n",
        "              connection.close();\n",
        "          }\n",
        "      };\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('useCam(\"{}\", {})'.format(url, quality))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json;url = json.load(sys.stdin)['tunnels'][0]['public_url'];print(url);f = open('url.txt', 'w');f.write(url);f.close()\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTS20Wl8RA_q",
        "outputId": "3937364d-9524-4630-871a-232a6871f2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-12 05:24:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.237.133.81, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  40.1MB/s    in 0.3s    \n",
            "\n",
            "2022-01-12 05:24:32 (40.1 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "Requirement already satisfied: bottle in /usr/local/lib/python3.7/dist-packages (0.12.19)\n",
            "Requirement already satisfied: bottle_websocket in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: gevent-websocket in /usr/local/lib/python3.7/dist-packages (from bottle_websocket) (0.10.1)\n",
            "Requirement already satisfied: bottle in /usr/local/lib/python3.7/dist-packages (from bottle_websocket) (0.12.19)\n",
            "Requirement already satisfied: gevent in /usr/local/lib/python3.7/dist-packages (from gevent-websocket->bottle_websocket) (21.12.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent->gevent-websocket->bottle_websocket) (1.1.2)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent->gevent-websocket->bottle_websocket) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent->gevent-websocket->bottle_websocket) (57.4.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent->gevent-websocket->bottle_websocket) (5.4.0)\n",
            "Requirement already satisfied: gevent in /usr/local/lib/python3.7/dist-packages (21.12.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent) (57.4.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent) (5.4.0)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent) (4.5.0)\n",
            "https://962b-34-67-119-98.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yimport numpy as np\n",
        "import cv2\n",
        "\n",
        "# Need adjustment\n",
        "lower_light_pink = np.array([168, 50, 50])\n",
        "upper_light_pink = np.array([188, 255, 255])\n",
        "\n",
        "_LOWER_COLOR = lower_light_pink\n",
        "_UPPER_COLOR = upper_light_pink\n",
        "\n",
        "def tracking():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    filter = ParticleFilter()\n",
        "    filter.initialize()\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # Threshold the HSV image to get only a color\n",
        "        mask = cv2.inRange(hsv, _LOWER_COLOR, _UPPER_COLOR)\n",
        "\n",
        "        # Start Tracking\n",
        "        y, x = filter.filtering(mask)\n",
        "        frame = cv2.circle(frame, (int(x), int(y)), 10, (255, 0, 0), -1)\n",
        "\n",
        "        # origin is upper left\n",
        "        frame_size = frame.shape\n",
        "        print (\"position_x_rate\")\n",
        "        print (x/frame_size[1])\n",
        "        print (\"position_y_rate\")\n",
        "        print (y/frame_size[0])\n",
        "\n",
        "        for i in range(filter.SAMPLEMAX):\n",
        "            frame = cv2.circle(frame, (int(filter.X[i]), int(filter.Y[i])), 2, (0, 0, 255), -1)\n",
        "        cv2.imshow(\"frame\", frame)\n",
        "\n",
        "        if cv2.waitKey(20) & 0xFF == 27:\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "class ParticleFilter:\n",
        "    def __init__(self):\n",
        "        self.SAMPLEMAX = 1000\n",
        "        # frame.shape\n",
        "        self.height, self.width = 480, 640\n",
        "\n",
        "    def initialize(self):\n",
        "        self.Y = np.random.random(self.SAMPLEMAX) * self.height\n",
        "        self.X = np.random.random(self.SAMPLEMAX) * self.width\n",
        "\n",
        "    # Need adjustment for tracking object velocity\n",
        "    def modeling(self):\n",
        "        self.Y += np.random.random(self.SAMPLEMAX) * 200 - 100 # 2:1\n",
        "        self.X += np.random.random(self.SAMPLEMAX) * 200 - 100\n",
        "\n",
        "    def normalize(self, weight):\n",
        "        return weight / np.sum(weight)\n",
        "\n",
        "    def resampling(self, weight):\n",
        "        index = np.arange(self.SAMPLEMAX)\n",
        "        sample = []\n",
        "\n",
        "        # choice by weight \n",
        "        for i in range(self.SAMPLEMAX):\n",
        "            idx = np.random.choice(index, p=weight)\n",
        "            sample.append(idx)\n",
        "        return sample\n",
        "\n",
        "    def calcLikelihood(self, image):\n",
        "        # white space tracking \n",
        "        mean, std = 250.0, 10.0\n",
        "        intensity = []\n",
        "\n",
        "        for i in range(self.SAMPLEMAX):\n",
        "            y, x = self.Y[i], self.X[i]\n",
        "            if y >= 0 and y < self.height and x >= 0 and x < self.width:\n",
        "                intensity.append(image[int(y),int(x)])\n",
        "            else:\n",
        "                intensity.append(-1)\n",
        "\n",
        "        # normal distribution\n",
        "        weights = 1.0 / np.sqrt(2 * np.pi * std) * np.exp(-(np.array(intensity) - mean)**2 /(2 * std**2))\n",
        "        weights[intensity == -1] = 0\n",
        "        weights = self.normalize(weights)\n",
        "        return weights\n",
        "\n",
        "    def filtering(self, image):\n",
        "        self.modeling()\n",
        "        weights = self.calcLikelihood(image)\n",
        "        index = self.resampling(weights)\n",
        "        self.Y = self.Y[index]\n",
        "        self.X = self.X[index]\n",
        "        \n",
        "        # return COG\n",
        "        return np.sum(self.Y) / float(len(self.Y)), np.sum(self.X) / float(len(self.X))"
      ],
      "metadata": {
        "id": "7adrW7tgTlCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import bottle\n",
        "import gevent\n",
        "from bottle.ext.websocket import GeventWebSocketServer\n",
        "from bottle.ext.websocket import websocket\n",
        "from multiprocessing import Pool\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import base64\n",
        "lower_color = np.array([170,170,90])\n",
        "upper_color = np.array([190,255,255])\n",
        "_LOWER_COLOR = lower_color\n",
        "_UPPER_COLOR = upper_color\n",
        "\n",
        "socket = bottle.Bottle()\n",
        "@socket.route('/', apply=[websocket])\n",
        "def wsbin(ws):\n",
        "    while True:\n",
        "        try:\n",
        "            #decode to image\n",
        "            img_str = ws.receive()\n",
        "            decimg = base64.b64decode(img_str.split(',')[1], validate=True)\n",
        "            decimg = Image.open(BytesIO(decimg))\n",
        "            decimg = np.array(decimg, dtype=np.uint8); \n",
        "            decimg = cv2.cvtColor(decimg, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            #############your process###############\n",
        "\n",
        "            cap = cv2.VideoCapture(0)\n",
        "\n",
        "            filter = ParticleFilter()\n",
        "            filter.initialize()\n",
        "\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "                # Threshold the HSV image to get only a color\n",
        "                mask = cv2.inRange(hsv, _LOWER_COLOR, _UPPER_COLOR)\n",
        "\n",
        "                # Start Tracking\n",
        "                y, x = filter.filtering(mask)\n",
        "                frame = cv2.circle(frame, (int(x), int(y)), 10, (255, 0, 0), -1)\n",
        "\n",
        "                # origin is upper left\n",
        "                frame_size = frame.shape\n",
        "                print (\"position_x_rate\")\n",
        "                print (x/frame_size[1])\n",
        "                print (\"position_y_rate\")\n",
        "                print (y/frame_size[0])\n",
        "\n",
        "                for i in range(filter.SAMPLEMAX):\n",
        "                    frame = cv2.circle(frame, (int(filter.X[i]), int(filter.Y[i])), 2, (0, 0, 255), -1)\n",
        "                cv2.imshow(\"frame\", frame)\n",
        "\n",
        "                if cv2.waitKey(20) & 0xFF == 27:\n",
        "                    break\n",
        "            cap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "            #############your process###############\n",
        "\n",
        "            #encode to string\n",
        "            _, encimg = cv2.imencode(\".jpg\", out_img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n",
        "            img_str = encimg.tostring()\n",
        "            img_str = \"data:image/jpeg;base64,\" + base64.b64encode(img_str).decode('utf-8')\n",
        "            ws.send(img_str)\n",
        "        except:\n",
        "            pass\n",
        "            #print(\"error\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # get ngrok url\n",
        "    f = open(\"url.txt\", \"r\")\n",
        "    url = f.read()\n",
        "    f.close()\n",
        "    url = \"wss\" + url[5:]\n",
        "    # prepare multiprocess\n",
        "    _pool = Pool(processes=2)\n",
        "    _pool.apply_async(use_cam, (url, 0.8))\n",
        "    socket.run(host='0.0.0.0', port=6006, server=GeventWebSocketServer)"
      ],
      "metadata": {
        "id": "OwtLDczhUXHI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8c822f7-3d10-452e-9bb4-428ea6c5ea25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start camera\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function useCam(url, quality) {\n",
              "      const div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "      //video element\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'None';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      //canvas for display. frame rate is depending on display size and jpeg quality.\n",
              "      display_size = 500 \n",
              "      const src_canvas = document.createElement('canvas');\n",
              "      src_canvas.width  = display_size;\n",
              "      src_canvas.height = display_size * video.videoHeight / video.videoWidth;\n",
              "      const src_canvasCtx = src_canvas.getContext('2d');\n",
              "      src_canvasCtx.translate(src_canvas.width, 0);\n",
              "      src_canvasCtx.scale(-1, 1);\n",
              "      div.appendChild(src_canvas);\n",
              "\n",
              "      const dst_canvas = document.createElement('canvas');\n",
              "      dst_canvas.width  = src_canvas.width;\n",
              "      dst_canvas.height = src_canvas.height;\n",
              "      const dst_canvasCtx = dst_canvas.getContext('2d');\n",
              "      div.appendChild(dst_canvas);\n",
              "\n",
              "      //exit button\n",
              "      const btn_div = document.createElement('div');\n",
              "      document.body.appendChild(btn_div);\n",
              "      const exit_btn = document.createElement('button');\n",
              "      exit_btn.textContent = 'Exit';\n",
              "      var exit_flg = true\n",
              "      exit_btn.onclick = function() {exit_flg = false};\n",
              "      btn_div.appendChild(exit_btn);\n",
              "\n",
              "      //log\n",
              "      let jsLog = function(abc) {\n",
              "        document.querySelector(\"#output-area\").appendChild(document.createTextNode(`${abc} `));\n",
              "      }\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      //for websocket connection.\n",
              "      var connection = 0\n",
              "      var flag_count = 0\n",
              "      var send_flg = false\n",
              "\n",
              "      // loop\n",
              "      _canvasUpdate();\n",
              "      async function _canvasUpdate() {\n",
              "            flag_count += 1\n",
              "\n",
              "            //wait until websocket launch\n",
              "            if (flag_count == 200){\n",
              "                connection = new WebSocket(url); \n",
              "                jsLog(\"Connect_start\")\n",
              "            }\n",
              "            if (flag_count == 300){\n",
              "                connection.onmessage = function(e) {\n",
              "                    var image = new Image()\n",
              "                    image.src = e.data;\n",
              "                    image.onload = function(){dst_canvasCtx.drawImage(image, 0, 0)}\n",
              "                    send_flg=false\n",
              "                };\n",
              "                jsLog(\"Set_recieve\")\n",
              "            }\n",
              "            if(flag_count > 400){\n",
              "                //resize to reduce file size\n",
              "                src_canvasCtx.drawImage(video, 0, 0, video.videoWidth, video.videoHeight, 0, 0, src_canvas.width, src_canvas.height);\n",
              "                const img = src_canvas.toDataURL('image/jpeg', quality);\n",
              "                if (send_flg==false){\n",
              "                    connection.send(img);\n",
              "                    send_flg = true\n",
              "                }\n",
              "            }\n",
              "          if (exit_flg){\n",
              "              requestAnimationFrame(_canvasUpdate);   \n",
              "          }else{\n",
              "              stream.getVideoTracks()[0].stop();\n",
              "              connection.close();\n",
              "          }\n",
              "      };\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Bottle v0.12.19 server starting up (using GeventWebSocketServer())...\n",
            "Listening on http://0.0.0.0:6006/\n",
            "Hit Ctrl-C to quit.\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 157, in _before_run_task\n",
            "    _sys.settrace(_get_thread_trace())\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 162, in _after_run_task\n",
            "    _sys.settrace(None)\n",
            "\n",
            "Process ForkPoolWorker-1:\n",
            "Process ForkPoolWorker-2:\n",
            "KeyboardInterrupt\n",
            "2022-01-12T05:26:34Z\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"<ipython-input-6-ee5388a8fe31>\", line 99, in use_cam\n",
            "    data = eval_js('useCam(\"{}\", {})'.format(url, quality))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\", line 40, in eval_js\n",
            "    return _message.read_reply_from_input(request_id, timeout_sec)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\", line 99, in read_reply_from_input\n",
            "    reply = _read_next_input_message()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\", line 45, in _read_next_input_message\n",
            "    _, reply = kernel.session.recv(stdin_socket, zmq.NOBLOCK)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\", line 803, in recv\n",
            "    msg_list = socket.recv_multipart(mode, copy=copy)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\", line 625, in recv_multipart\n",
            "    parts = [self.recv(flags, copy=copy, track=track)]\n",
            "  File \"zmq/backend/cython/socket.pyx\", line 781, in zmq.backend.cython.socket.Socket.recv\n",
            "  File \"zmq/backend/cython/socket.pyx\", line 817, in zmq.backend.cython.socket.Socket.recv\n",
            "  File \"zmq/backend/cython/socket.pyx\", line 186, in zmq.backend.cython.socket._recv_copy\n",
            "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CAXuGdj2hUXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OyuoTB0wnPSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}